{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b2e2dd4-ef28-40a7-89a6-67cbff7f7b8d",
   "metadata": {},
   "source": [
    "# [Langchain RAG](https://python.langchain.com/docs/tutorials/rag/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf48fd58-fa56-42e9-b91e-0f0674ef7f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from langchain_ollama import OllamaLLM\n",
    "#llm = OllamaLLM(model=\"llama3.2:latest\")\n",
    "\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "llm = ChatOllama(\n",
    "   model=\"llama3.2:latest\",\n",
    "   temperature=0,\n",
    "   # other params...\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "981cd20e-b287-46a8-8881-ae483c5fedac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaEmbeddings\n",
    "\n",
    "embeddings = OllamaEmbeddings(model=\"llama3.2:latest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1fff7bc8-6747-4281-aa61-8d729062a00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "\n",
    "vector_store = Chroma(\n",
    "    collection_name=\"example_collection\",\n",
    "    embedding_function=embeddings,\n",
    "    persist_directory=\"./chroma_langchain_db\",  # Where to save data locally, remove if not necessary\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1fff60f5-171a-4689-b346-04c2089d1d8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "import bs4\n",
    "from langchain import hub\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_core.documents import Document\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langgraph.graph import START, StateGraph\n",
    "from typing_extensions import List, TypedDict\n",
    "\n",
    "# Load and chunk contents of the blog\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "20e14352-739b-4a40-b5ca-b68c4ec6303f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "30325976-92b7-41e8-87e0-7f70b3942057",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n      LLM Powered Autonomous Agents\\n    \\nDate: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\\n\\n\\nBuilding agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview#\\nIn'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0].page_content[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d95df32-5c19-4778-a9d1-50283dcb0555",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "all_splits = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d6b87103-2dfd-4198-9853-54c54ce483e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4cad7362-9c4a-4be8-a5fd-ddf33693c7a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='LLM Powered Autonomous Agents\\n    \\nDate: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\\n\\n\\nBuilding agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview#\\nIn a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_splits[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8a46d05a-8c68-491f-9195-97526e80702d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Memory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\n\\nOverview of a LLM-powered autonomous agent system.')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_splits[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "92343185-2e0d-4f86-b7c6-6cc02323ebe0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Component One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.\\nTree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_splits[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8919d233-5e56-44ee-8cf1-b6353b6985f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = vector_store.add_documents(documents=all_splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c3a3fe-5a29-4aa6-88b9-98d181d5350b",
   "metadata": {},
   "source": [
    "great. works!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c8e5ec4-5284-4b93-9106-85fb92228308",
   "metadata": {},
   "source": [
    "deviate from tutorial. Use [Building a RAG Application with LangChain: Example Code](https://vitalflux.com/building-llm-rag-application-langchain-example-code/#google_vignette)    to set up prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f1817a54-304d-482d-9bc9-fc1cdbabb3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    " \n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"Use the following pieces of context to answer the question at the end.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "Use three sentences maximum and keep the answer as concise as possible.\n",
    "Always say \"thanks for asking!\" at the end of the answer.\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Helpful Answer:\"\"\"\n",
    "custom_rag_prompt = PromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "347fb6df-5a32-4428-a8d5-93eed8dfbd25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up state\n",
    "# Define state for application\n",
    "class State(TypedDict):\n",
    "    question: str\n",
    "    context: List[Document]\n",
    "    answer: str\n",
    "\n",
    "\n",
    "# Define application steps\n",
    "def retrieve(state: State):\n",
    "    retrieved_docs = vector_store.similarity_search(state[\"question\"])\n",
    "    return {\"context\": retrieved_docs}\n",
    "\n",
    "\n",
    "def generate(state: State):\n",
    "    docs_content = \"\\n\\n\".join(doc.page_content for doc in state[\"context\"])\n",
    "    messages = custom_rag_prompt.invoke({\"question\": state[\"question\"], \"context\": docs_content})\n",
    "    response = llm.invoke(messages)\n",
    "    return {\"answer\": response.content}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2cb78012-f795-4bcb-89a3-e8bb4d548df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile application and test\n",
    "graph_builder = StateGraph(State).add_sequence([retrieve, generate])\n",
    "graph_builder.add_edge(START, \"retrieve\")\n",
    "graph = graph_builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04793ba5-ad59-4d24-a011-30cb78812bdf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "77d0a050-2829-44b8-8c87-00fa966a583b",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = graph.invoke({\"question\": \"What is Task Decomposition?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7eca019a-b4ec-4ef7-84c2-8cb811fa49d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'What is Task Decomposition?',\n",
       " 'context': [Document(id='ecb1bd49-1b93-4991-941c-16f7917791e7', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Resources:\\n1. Internet access for searches and information gathering.\\n2. Long Term memory management.\\n3. GPT-3.5 powered Agents for delegation of simple tasks.\\n4. File output.\\n\\nPerformance Evaluation:\\n1. Continuously review and analyze your actions to ensure you are performing to the best of your abilities.\\n2. Constructively self-criticize your big-picture behavior constantly.\\n3. Reflect on past decisions and strategies to refine your approach.\\n4. Every command has a cost, so be smart and efficient. Aim to complete tasks in the least number of steps.'),\n",
       "  Document(id='6a635182-4069-4227-b150-401bd92eb10e', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Resources:\\n1. Internet access for searches and information gathering.\\n2. Long Term memory management.\\n3. GPT-3.5 powered Agents for delegation of simple tasks.\\n4. File output.\\n\\nPerformance Evaluation:\\n1. Continuously review and analyze your actions to ensure you are performing to the best of your abilities.\\n2. Constructively self-criticize your big-picture behavior constantly.\\n3. Reflect on past decisions and strategies to refine your approach.\\n4. Every command has a cost, so be smart and efficient. Aim to complete tasks in the least number of steps.'),\n",
       "  Document(id='6a864b0b-37c9-411b-955c-28c14ea45b98', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='MRKL (Karpas et al. 2022), short for “Modular Reasoning, Knowledge and Language”, is a neuro-symbolic architecture for autonomous agents. A MRKL system is proposed to contain a collection of “expert” modules and the general-purpose LLM works as a router to route inquiries to the best suitable expert module. These modules can be neural (e.g. deep learning models) or symbolic (e.g. math calculator, currency converter, weather API).\\nThey did an experiment on fine-tuning LLM to call a calculator, using arithmetic as a test case. Their experiments showed that it was harder to solve verbal math problems than explicitly stated math problems because LLMs (7B Jurassic1-large model) failed to extract the right arguments for the basic arithmetic reliably. The results highlight when the external symbolic tools can work reliably, knowing when to and how to use the tools are crucial, determined by the LLM capability.'),\n",
       "  Document(id='4dd19c46-371a-4683-813b-52909cfb6bc1', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='MRKL (Karpas et al. 2022), short for “Modular Reasoning, Knowledge and Language”, is a neuro-symbolic architecture for autonomous agents. A MRKL system is proposed to contain a collection of “expert” modules and the general-purpose LLM works as a router to route inquiries to the best suitable expert module. These modules can be neural (e.g. deep learning models) or symbolic (e.g. math calculator, currency converter, weather API).\\nThey did an experiment on fine-tuning LLM to call a calculator, using arithmetic as a test case. Their experiments showed that it was harder to solve verbal math problems than explicitly stated math problems because LLMs (7B Jurassic1-large model) failed to extract the right arguments for the basic arithmetic reliably. The results highlight when the external symbolic tools can work reliably, knowing when to and how to use the tools are crucial, determined by the LLM capability.')],\n",
       " 'answer': \"I don't know what Task Decomposition is based on the provided context. \\n\\nThanks for asking!\"}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "44008831-cd22-4e07-ad55-b3e3e6afe550",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'retrieve': {'context': [Document(id='ecb1bd49-1b93-4991-941c-16f7917791e7', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Resources:\\n1. Internet access for searches and information gathering.\\n2. Long Term memory management.\\n3. GPT-3.5 powered Agents for delegation of simple tasks.\\n4. File output.\\n\\nPerformance Evaluation:\\n1. Continuously review and analyze your actions to ensure you are performing to the best of your abilities.\\n2. Constructively self-criticize your big-picture behavior constantly.\\n3. Reflect on past decisions and strategies to refine your approach.\\n4. Every command has a cost, so be smart and efficient. Aim to complete tasks in the least number of steps.'), Document(id='6a635182-4069-4227-b150-401bd92eb10e', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Resources:\\n1. Internet access for searches and information gathering.\\n2. Long Term memory management.\\n3. GPT-3.5 powered Agents for delegation of simple tasks.\\n4. File output.\\n\\nPerformance Evaluation:\\n1. Continuously review and analyze your actions to ensure you are performing to the best of your abilities.\\n2. Constructively self-criticize your big-picture behavior constantly.\\n3. Reflect on past decisions and strategies to refine your approach.\\n4. Every command has a cost, so be smart and efficient. Aim to complete tasks in the least number of steps.'), Document(id='6a864b0b-37c9-411b-955c-28c14ea45b98', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='MRKL (Karpas et al. 2022), short for “Modular Reasoning, Knowledge and Language”, is a neuro-symbolic architecture for autonomous agents. A MRKL system is proposed to contain a collection of “expert” modules and the general-purpose LLM works as a router to route inquiries to the best suitable expert module. These modules can be neural (e.g. deep learning models) or symbolic (e.g. math calculator, currency converter, weather API).\\nThey did an experiment on fine-tuning LLM to call a calculator, using arithmetic as a test case. Their experiments showed that it was harder to solve verbal math problems than explicitly stated math problems because LLMs (7B Jurassic1-large model) failed to extract the right arguments for the basic arithmetic reliably. The results highlight when the external symbolic tools can work reliably, knowing when to and how to use the tools are crucial, determined by the LLM capability.'), Document(id='4dd19c46-371a-4683-813b-52909cfb6bc1', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='MRKL (Karpas et al. 2022), short for “Modular Reasoning, Knowledge and Language”, is a neuro-symbolic architecture for autonomous agents. A MRKL system is proposed to contain a collection of “expert” modules and the general-purpose LLM works as a router to route inquiries to the best suitable expert module. These modules can be neural (e.g. deep learning models) or symbolic (e.g. math calculator, currency converter, weather API).\\nThey did an experiment on fine-tuning LLM to call a calculator, using arithmetic as a test case. Their experiments showed that it was harder to solve verbal math problems than explicitly stated math problems because LLMs (7B Jurassic1-large model) failed to extract the right arguments for the basic arithmetic reliably. The results highlight when the external symbolic tools can work reliably, knowing when to and how to use the tools are crucial, determined by the LLM capability.')]}}\n",
      "\n",
      "----------------\n",
      "\n",
      "{'generate': {'answer': \"I don't know what Task Decomposition is based on the provided context. \\n\\nThanks for asking!\"}}\n",
      "\n",
      "----------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for step in graph.stream({\"question\": \"What is Task Decomposition?\"}, stream_mode=\"updates\"):\n",
    "    print(f\"{step}\\n\\n----------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216e681b-f2b2-40a0-b94b-1d95783788c9",
   "metadata": {},
   "source": [
    "# Query Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f09d45a2-4cf4-47b6-a7f9-368be71ce14a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/',\n",
       " 'section': 'beginning'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_documents = len(all_splits)\n",
    "third = total_documents // 3\n",
    "\n",
    "for i, document in enumerate(all_splits):\n",
    "    if i < third:\n",
    "        document.metadata[\"section\"] = \"beginning\"\n",
    "    elif i < 2 * third:\n",
    "        document.metadata[\"section\"] = \"middle\"\n",
    "    else:\n",
    "        document.metadata[\"section\"] = \"end\"\n",
    "\n",
    "\n",
    "all_splits[0].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "74a24b2d-ecd0-49a0-a907-bb6e928c1fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "\n",
    "vector_store = InMemoryVectorStore(embeddings)\n",
    "x = vector_store.add_documents(all_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7ba3e11f-5b71-4202-abd7-98fc31e01390",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "\n",
    "from typing_extensions import Annotated\n",
    "\n",
    "\n",
    "class Search(TypedDict):\n",
    "    \"\"\"Search query.\"\"\"\n",
    "\n",
    "    query: Annotated[str, ..., \"Search query to run.\"]\n",
    "    section: Annotated[\n",
    "        Literal[\"beginning\", \"middle\", \"end\"],\n",
    "        ...,\n",
    "        \"Section to query.\",\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "841298c2-e994-4e4c-a391-1baef082d85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class State(TypedDict):\n",
    "    question: str\n",
    "    query: Search\n",
    "    context: List[Document]\n",
    "    answer: str\n",
    "\n",
    "\n",
    "def analyze_query(state: State):\n",
    "    structured_llm = llm.with_structured_output(Search)\n",
    "    query = structured_llm.invoke(state[\"question\"])\n",
    "    return {\"query\": query}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bd85df2e-4f91-4334-a1e8-be50a8a7fc45",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    " \n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    " \n",
    "Question: {question}\n",
    "\"\"\"\n",
    " \n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "def retrieve(state: State):\n",
    "    query = state[\"query\"]\n",
    "    retrieved_docs = vector_store.similarity_search(\n",
    "        query[\"query\"],\n",
    "        filter=lambda doc: doc.metadata.get(\"section\") == query[\"section\"],\n",
    "    )\n",
    "    return {\"context\": retrieved_docs}\n",
    "\n",
    "\n",
    "def generate(state: State):\n",
    "    docs_content = \"\\n\\n\".join(doc.page_content for doc in state[\"context\"])\n",
    "    messages = prompt.invoke({\"question\": state[\"question\"], \"context\": docs_content})\n",
    "    response = llm.invoke(messages)\n",
    "    return {\"answer\": response.content}\n",
    "\n",
    "\n",
    "graph_builder = StateGraph(State).add_sequence([analyze_query, retrieve, generate])\n",
    "graph_builder.add_edge(START, \"analyze_query\")\n",
    "graph = graph_builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8c813709-feed-41c6-978a-45989e238e3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "159ae200-48f4-430e-94ab-e63be746de81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'analyze_query': {'query': {'query': 'Task Decomposition', 'section': 'end'}}}\n",
      "\n",
      "----------------\n",
      "\n",
      "{'retrieve': {'context': [Document(id='fe36d63e-7540-436d-bfd0-f72627aacef4', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'section': 'end'}, page_content='Finite context length: The restricted context capacity limits the inclusion of historical information, detailed instructions, API call context, and responses. The design of the system has to work with this limited communication bandwidth, while mechanisms like self-reflection to learn from past mistakes would benefit a lot from long or infinite context windows. Although vector stores and retrieval can provide access to a larger knowledge pool, their representation power is not as powerful as full attention.\\n\\n\\nChallenges in long-term planning and task decomposition: Planning over a lengthy history and effectively exploring the solution space remain challenging. LLMs struggle to adjust plans when faced with unexpected errors, making them less robust compared to humans who learn from trial and error.'), Document(id='239b6875-49de-45c6-bea5-52d78c5ac97d', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'section': 'end'}, page_content='}\\n]\\nChallenges#\\nAfter going through key ideas and demos of building LLM-centered agents, I start to see a couple common limitations:'), Document(id='5a27d22f-7757-4349-949c-71cdd8006a5c', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'section': 'end'}, page_content='}\\n]\\nThen after these clarification, the agent moved into the code writing mode with a different system message.\\nSystem message:'), Document(id='13191a91-1ba1-4882-af33-0908e4aa244f', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'section': 'end'}, page_content='[6] Google Blog. “Announcing ScaNN: Efficient Vector Similarity Search” July 28, 2020.\\n[7] https://chat.openai.com/share/46ff149e-a4c7-4dd7-a800-fc4a642ea389\\n[8] Shinn & Labash. “Reflexion: an autonomous agent with dynamic memory and self-reflection” arXiv preprint arXiv:2303.11366 (2023).\\n[9] Laskin et al. “In-context Reinforcement Learning with Algorithm Distillation” ICLR 2023.\\n[10] Karpas et al. “MRKL Systems A modular, neuro-symbolic architecture that combines large language models, external knowledge sources and discrete reasoning.” arXiv preprint arXiv:2205.00445 (2022).\\n[11] Nakano et al. “Webgpt: Browser-assisted question-answering with human feedback.” arXiv preprint arXiv:2112.09332 (2021).\\n[12] Parisi et al. “TALM: Tool Augmented Language Models”\\n[13] Schick et al. “Toolformer: Language Models Can Teach Themselves to Use Tools.” arXiv preprint arXiv:2302.04761 (2023).\\n[14] Weaviate Blog. Why is Vector Search so fast? Sep 13, 2022.')]}}\n",
      "\n",
      "----------------\n",
      "\n",
      "{'generate': {'answer': 'The text doesn\\'t explicitly state what the end of the post says about task decomposition. However, it mentions that \"Planning over a lengthy history and effectively exploring the solution space remain challenging\" as one of the challenges in long-term planning and task decomposition for LLMs.'}}\n",
      "\n",
      "----------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for step in graph.stream(\n",
    "    {\"question\": \"What does the end of the post say about Task Decomposition?\"},\n",
    "    stream_mode=\"updates\",\n",
    "):\n",
    "    print(f\"{step}\\n\\n----------------\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2e1dc980-2272-4cb9-9682-7fa1115b2d5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'analyze_query': {'query': {'query': 'Task Decomposition', 'section': 'beginning'}}}\n",
      "\n",
      "----------------\n",
      "\n",
      "{'retrieve': {'context': [Document(id='f04d5b62-3345-4dac-bae8-a425e939879c', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'section': 'beginning'}, page_content='MRKL (Karpas et al. 2022), short for “Modular Reasoning, Knowledge and Language”, is a neuro-symbolic architecture for autonomous agents. A MRKL system is proposed to contain a collection of “expert” modules and the general-purpose LLM works as a router to route inquiries to the best suitable expert module. These modules can be neural (e.g. deep learning models) or symbolic (e.g. math calculator, currency converter, weather API).\\nThey did an experiment on fine-tuning LLM to call a calculator, using arithmetic as a test case. Their experiments showed that it was harder to solve verbal math problems than explicitly stated math problems because LLMs (7B Jurassic1-large model) failed to extract the right arguments for the basic arithmetic reliably. The results highlight when the external symbolic tools can work reliably, knowing when to and how to use the tools are crucial, determined by the LLM capability.'), Document(id='1d45b2e6-f6cc-40c5-a9a3-710bdc9a1055', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'section': 'beginning'}, page_content='The paper hypothesizes that any algorithm that generates a set of learning histories can be distilled into a neural network by performing behavioral cloning over actions. The history data is generated by a set of source policies, each trained for a specific task. At the training stage, during each RL run, a random task is sampled and a subsequence of multi-episode history is used for training, such that the learned policy is task-agnostic.\\nIn reality, the model has limited context window length, so episodes should be short enough to construct multi-episode history. Multi-episodic contexts of 2-4 episodes are necessary to learn a near-optimal in-context RL algorithm. The emergence of in-context RL requires long enough context.'), Document(id='a5ccef40-fd93-407d-a116-2fd684223a07', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'section': 'beginning'}, page_content='Chain of Hindsight (CoH; Liu et al. 2023) encourages the model to improve on its own outputs by explicitly presenting it with a sequence of past outputs, each annotated with feedback. Human feedback data is a collection of $D_h = \\\\{(x, y_i , r_i , z_i)\\\\}_{i=1}^n$, where $x$ is the prompt, each $y_i$ is a model completion, $r_i$ is the human rating of $y_i$, and $z_i$ is the corresponding human-provided hindsight feedback. Assume the feedback tuples are ranked by reward, $r_n \\\\geq r_{n-1} \\\\geq \\\\dots \\\\geq r_1$ The process is supervised fine-tuning where the data is a sequence in the form of $\\\\tau_h = (x, z_i, y_i, z_j, y_j, \\\\dots, z_n, y_n)$, where $\\\\leq i \\\\leq j \\\\leq n$. The model is finetuned to only predict $y_n$ where conditioned on the sequence prefix, such that the model can self-reflect to produce better output based on the feedback sequence. The model can optionally receive multiple rounds of instructions with human annotators at test time.'), Document(id='f17ef64d-72f6-43dc-8d85-08a32f1e2d3e', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'section': 'beginning'}, page_content='FAISS (Facebook AI Similarity Search): It operates on the assumption that in high dimensional space, distances between nodes follow a Gaussian distribution and thus there should exist clustering of data points. FAISS applies vector quantization by partitioning the vector space into clusters and then refining the quantization within clusters. Search first looks for cluster candidates with coarse quantization and then further looks into each cluster with finer quantization.\\nScaNN (Scalable Nearest Neighbors): The main innovation in ScaNN is anisotropic vector quantization. It quantizes a data point $x_i$ to $\\\\tilde{x}_i$ such that the inner product $\\\\langle q, x_i \\\\rangle$ is as similar to the original distance of $\\\\angle q, \\\\tilde{x}_i$ as possible, instead of picking the closet quantization centroid points.')]}}\n",
      "\n",
      "----------------\n",
      "\n",
      "{'generate': {'answer': \"The beginning of the post doesn't mention task decomposition explicitly. However, it does describe a neuro-symbolic architecture called MRKL that uses a general-purpose LLM as a router to route inquiries to the best suitable expert module for specific tasks (e.g., arithmetic). This suggests that the system is designed to handle multiple tasks and can be decomposed into smaller subtasks or modules, but it doesn't provide further information about task decomposition itself.\"}}\n",
      "\n",
      "----------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for step in graph.stream(\n",
    "    {\"question\": \"What does the beginning of the post say about Task Decomposition?\"},\n",
    "    stream_mode=\"updates\",\n",
    "):\n",
    "    print(f\"{step}\\n\\n----------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ec1044-7ff8-4c21-bbdd-12912b3a1a34",
   "metadata": {},
   "source": [
    "ok. so analyze query calls with \"Search\" as the method. Search pulls out the query and the section. Retrieve then gets things that match only that section and\n",
    "uses them as context for the final search."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
